{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_plot(estimator, plot=False):\n",
    "    df['Predicted_Signal'] = estimator.predict(X)\n",
    "    df.Predicted_Signal[df.Predicted_Signal > 0.5] = 1\n",
    "    df.Predicted_Signal[df.Predicted_Signal < 0.5] = -1\n",
    "    Cumulative_returns = np.cumsum(df[split:]['Returns'])\n",
    "    df['Startegy_returns'] = df['Returns']* df['Predicted_Signal'].shift(1)\n",
    "    Cumulative_Strategy_returns = np.cumsum(df[split:]['Startegy_returns'])\n",
    "    print('Return: {}%'.format(round(Cumulative_Strategy_returns[-1]*100, 2)))\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.plot(Cumulative_returns, color='r',label = 'Returns')\n",
    "        plt.plot(Cumulative_Strategy_returns, color='g', label = 'Strategy Returns')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "def accuracy_plot(estimator):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(estimator.history.history['acc'])\n",
    "    plt.plot(estimator.history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(estimator.history.history['loss'])\n",
    "    plt.plot(estimator.history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def compare_plot(model_1, model_2):\n",
    "    plt.plot(model_1.history.history['val_loss'], 'r', model_2.history.history['val_loss'], 'b')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Validation Score')\n",
    "    plt.legend(['first', 'second'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, Activation, TimeDistributed\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD, Adagrad, Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data3_M1.csv', parse_dates=['Datetime'], index_col='Datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 5807496 entries, 2003-08-04 03:19:00 to 2019-02-06 23:59:00\n",
      "Data columns (total 26 columns):\n",
      "Target                    int64\n",
      "body_candle               float64\n",
      "high_low                  float64\n",
      "macd                      float64\n",
      "macdsignal                float64\n",
      "macdhist                  float64\n",
      "macd-cross                int64\n",
      "range_ma35                float64\n",
      "ma35_valid                int64\n",
      "ma200_valid               int64\n",
      "35_200_cross              int64\n",
      "Returns                   float64\n",
      "ATR                       float64\n",
      "ATR_diff                  float64\n",
      "ADX                       float64\n",
      "ADX_diff                  float64\n",
      "CCI                       float64\n",
      "CCI_diff                  float64\n",
      "MOM                       float64\n",
      "MOM_diff                  float64\n",
      "RSI                       float64\n",
      "RSI_diff                  float64\n",
      "Linear_regression_diff    float64\n",
      "Linear_angle_diff         float64\n",
      "Linear_slope_diff         float64\n",
      "Linear_intercept_diff     float64\n",
      "dtypes: float64(21), int64(5)\n",
      "memory usage: 1.2 GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Target', axis=1).values\n",
    "y = df.Target.values\n",
    "\n",
    "#split = int(len(df) * 0.60)\n",
    "#X_train = X[:split]\n",
    "#X_test = X[split:]\n",
    "#y_train = y[:split]\n",
    "#y_test = y[split:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ballmdr/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(patience=5, monitor='val_acc')\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "ada = Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1626098 samples, validate on 696900 samples\n",
      "Epoch 1/200\n",
      "1626098/1626098 [==============================] - 97s 60us/step - loss: 0.2473 - acc: 0.5479 - val_loss: 0.2457 - val_acc: 0.5569\n",
      "Epoch 2/200\n",
      "   2784/1626098 [..............................] - ETA: 2:03 - loss: 0.2451 - acc: 0.5636"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ballmdr/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/callbacks.py:569: RuntimeWarning: Early stopping conditioned on metric `auc_roc` which is not available. Available metrics are: val_loss,val_acc,loss,acc\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1626098/1626098 [==============================] - 96s 59us/step - loss: 0.2454 - acc: 0.5547 - val_loss: 0.2445 - val_acc: 0.5580\n",
      "Epoch 3/200\n",
      "1626098/1626098 [==============================] - 97s 60us/step - loss: 0.2449 - acc: 0.5556 - val_loss: 0.2446 - val_acc: 0.5579\n",
      "Epoch 4/200\n",
      "1626098/1626098 [==============================] - 97s 60us/step - loss: 0.2447 - acc: 0.5566 - val_loss: 0.2441 - val_acc: 0.5589\n",
      "Epoch 5/200\n",
      "1626098/1626098 [==============================] - 97s 60us/step - loss: 0.2445 - acc: 0.5569 - val_loss: 0.2442 - val_acc: 0.5593\n",
      "Epoch 6/200\n",
      "1626098/1626098 [==============================] - 97s 60us/step - loss: 0.2444 - acc: 0.5576 - val_loss: 0.2438 - val_acc: 0.5595\n",
      "Epoch 7/200\n",
      "1626098/1626098 [==============================] - 98s 60us/step - loss: 0.2443 - acc: 0.5575 - val_loss: 0.2436 - val_acc: 0.5601\n",
      "Epoch 8/200\n",
      "1626098/1626098 [==============================] - 98s 60us/step - loss: 0.2442 - acc: 0.5579 - val_loss: 0.2437 - val_acc: 0.5597\n",
      "Epoch 9/200\n",
      "1626098/1626098 [==============================] - 98s 60us/step - loss: 0.2442 - acc: 0.5579 - val_loss: 0.2437 - val_acc: 0.5599\n",
      "Epoch 10/200\n",
      "1626098/1626098 [==============================] - 98s 60us/step - loss: 0.2441 - acc: 0.5583 - val_loss: 0.2439 - val_acc: 0.5588\n",
      "Epoch 11/200\n",
      "1626098/1626098 [==============================] - 98s 60us/step - loss: 0.2441 - acc: 0.5583 - val_loss: 0.2436 - val_acc: 0.5593\n",
      "Epoch 12/200\n",
      "1626098/1626098 [==============================] - 98s 60us/step - loss: 0.2440 - acc: 0.5582 - val_loss: 0.2436 - val_acc: 0.5597\n",
      "Epoch 13/200\n",
      "1626098/1626098 [==============================] - 98s 60us/step - loss: 0.2440 - acc: 0.5584 - val_loss: 0.2435 - val_acc: 0.5604\n",
      "Epoch 14/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2440 - acc: 0.5587 - val_loss: 0.2435 - val_acc: 0.5602\n",
      "Epoch 15/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2440 - acc: 0.5590 - val_loss: 0.2436 - val_acc: 0.5606\n",
      "Epoch 16/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2439 - acc: 0.5583 - val_loss: 0.2436 - val_acc: 0.5604\n",
      "Epoch 17/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2439 - acc: 0.5590 - val_loss: 0.2435 - val_acc: 0.5609\n",
      "Epoch 18/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2439 - acc: 0.5589 - val_loss: 0.2433 - val_acc: 0.5608\n",
      "Epoch 19/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2438 - acc: 0.5590 - val_loss: 0.2434 - val_acc: 0.5606\n",
      "Epoch 20/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2439 - acc: 0.5589 - val_loss: 0.2434 - val_acc: 0.5598\n",
      "Epoch 21/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2438 - acc: 0.5590 - val_loss: 0.2435 - val_acc: 0.5595\n",
      "Epoch 22/200\n",
      "1626098/1626098 [==============================] - 100s 62us/step - loss: 0.2438 - acc: 0.5590 - val_loss: 0.2434 - val_acc: 0.5611\n",
      "Epoch 23/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2438 - acc: 0.5594 - val_loss: 0.2433 - val_acc: 0.5609\n",
      "Epoch 24/200\n",
      "1626098/1626098 [==============================] - 94s 58us/step - loss: 0.2438 - acc: 0.5594 - val_loss: 0.2433 - val_acc: 0.5611\n",
      "Epoch 25/200\n",
      "1626098/1626098 [==============================] - 94s 58us/step - loss: 0.2438 - acc: 0.5592 - val_loss: 0.2433 - val_acc: 0.5610\n",
      "Epoch 26/200\n",
      "1626098/1626098 [==============================] - 94s 58us/step - loss: 0.2438 - acc: 0.5590 - val_loss: 0.2435 - val_acc: 0.5605\n",
      "Epoch 27/200\n",
      "1626098/1626098 [==============================] - 100s 61us/step - loss: 0.2437 - acc: 0.5595 - val_loss: 0.2434 - val_acc: 0.5601\n",
      "Epoch 28/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2437 - acc: 0.5593 - val_loss: 0.2433 - val_acc: 0.5609\n",
      "Epoch 29/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2437 - acc: 0.5591 - val_loss: 0.2433 - val_acc: 0.5611\n",
      "Epoch 30/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2437 - acc: 0.5593 - val_loss: 0.2432 - val_acc: 0.5609\n",
      "Epoch 31/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2437 - acc: 0.5595 - val_loss: 0.2431 - val_acc: 0.5610\n",
      "Epoch 32/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2437 - acc: 0.5597 - val_loss: 0.2433 - val_acc: 0.5615\n",
      "Epoch 33/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2437 - acc: 0.5593 - val_loss: 0.2432 - val_acc: 0.5608\n",
      "Epoch 34/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2437 - acc: 0.5595 - val_loss: 0.2432 - val_acc: 0.5612\n",
      "Epoch 35/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2437 - acc: 0.5595 - val_loss: 0.2432 - val_acc: 0.5616\n",
      "Epoch 36/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2436 - acc: 0.5599 - val_loss: 0.2431 - val_acc: 0.5612\n",
      "Epoch 37/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2437 - acc: 0.5597 - val_loss: 0.2434 - val_acc: 0.5609\n",
      "Epoch 38/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2436 - acc: 0.5595 - val_loss: 0.2433 - val_acc: 0.5609\n",
      "Epoch 39/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2436 - acc: 0.5598 - val_loss: 0.2431 - val_acc: 0.5610\n",
      "Epoch 40/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2436 - acc: 0.5599 - val_loss: 0.2433 - val_acc: 0.5603\n",
      "Epoch 41/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2436 - acc: 0.5598 - val_loss: 0.2431 - val_acc: 0.5611\n",
      "Epoch 42/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2436 - acc: 0.5596 - val_loss: 0.2434 - val_acc: 0.5610\n",
      "Epoch 43/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2436 - acc: 0.5598 - val_loss: 0.2433 - val_acc: 0.5604\n",
      "Epoch 44/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2436 - acc: 0.5595 - val_loss: 0.2432 - val_acc: 0.5611\n",
      "Epoch 45/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2436 - acc: 0.5597 - val_loss: 0.2432 - val_acc: 0.5614\n",
      "Epoch 46/200\n",
      "1626098/1626098 [==============================] - 100s 61us/step - loss: 0.2436 - acc: 0.5598 - val_loss: 0.2432 - val_acc: 0.5611\n",
      "Epoch 47/200\n",
      "1626098/1626098 [==============================] - 102s 63us/step - loss: 0.2436 - acc: 0.5597 - val_loss: 0.2433 - val_acc: 0.5607\n",
      "Epoch 48/200\n",
      "1626098/1626098 [==============================] - 100s 62us/step - loss: 0.2436 - acc: 0.5597 - val_loss: 0.2433 - val_acc: 0.5600\n",
      "Epoch 49/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2435 - acc: 0.5600 - val_loss: 0.2434 - val_acc: 0.5616\n",
      "Epoch 50/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2435 - acc: 0.5603 - val_loss: 0.2432 - val_acc: 0.5613\n",
      "Epoch 51/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2435 - acc: 0.5600 - val_loss: 0.2433 - val_acc: 0.5614\n",
      "Epoch 52/200\n",
      "1626098/1626098 [==============================] - 100s 61us/step - loss: 0.2435 - acc: 0.5603 - val_loss: 0.2432 - val_acc: 0.5611\n",
      "Epoch 53/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2435 - acc: 0.5603 - val_loss: 0.2432 - val_acc: 0.5615\n",
      "Epoch 54/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2435 - acc: 0.5601 - val_loss: 0.2433 - val_acc: 0.5614\n",
      "Epoch 55/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2435 - acc: 0.5599 - val_loss: 0.2431 - val_acc: 0.5614\n",
      "Epoch 56/200\n",
      "1626098/1626098 [==============================] - 98s 60us/step - loss: 0.2435 - acc: 0.5598 - val_loss: 0.2431 - val_acc: 0.5613\n",
      "Epoch 57/200\n",
      "1626098/1626098 [==============================] - 98s 60us/step - loss: 0.2436 - acc: 0.5602 - val_loss: 0.2431 - val_acc: 0.5600\n",
      "Epoch 58/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2435 - acc: 0.5601 - val_loss: 0.2431 - val_acc: 0.5615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2435 - acc: 0.5600 - val_loss: 0.2430 - val_acc: 0.5611\n",
      "Epoch 60/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2435 - acc: 0.5597 - val_loss: 0.2430 - val_acc: 0.5613\n",
      "Epoch 61/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2435 - acc: 0.5602 - val_loss: 0.2431 - val_acc: 0.5615\n",
      "Epoch 62/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2435 - acc: 0.5602 - val_loss: 0.2431 - val_acc: 0.5617\n",
      "Epoch 63/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2435 - acc: 0.5601 - val_loss: 0.2431 - val_acc: 0.5618\n",
      "Epoch 64/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2435 - acc: 0.5602 - val_loss: 0.2431 - val_acc: 0.5610\n",
      "Epoch 65/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2435 - acc: 0.5603 - val_loss: 0.2431 - val_acc: 0.5617\n",
      "Epoch 66/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2435 - acc: 0.5603 - val_loss: 0.2431 - val_acc: 0.5614\n",
      "Epoch 67/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2435 - acc: 0.5602 - val_loss: 0.2433 - val_acc: 0.5615\n",
      "Epoch 68/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2435 - acc: 0.5602 - val_loss: 0.2431 - val_acc: 0.5610\n",
      "Epoch 69/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2435 - acc: 0.5604 - val_loss: 0.2433 - val_acc: 0.5618\n",
      "Epoch 70/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2435 - acc: 0.5602 - val_loss: 0.2431 - val_acc: 0.5609\n",
      "Epoch 71/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2435 - acc: 0.5603 - val_loss: 0.2431 - val_acc: 0.5613\n",
      "Epoch 72/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2435 - acc: 0.5605 - val_loss: 0.2432 - val_acc: 0.5610\n",
      "Epoch 73/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2435 - acc: 0.5605 - val_loss: 0.2432 - val_acc: 0.5617\n",
      "Epoch 74/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2434 - acc: 0.5604 - val_loss: 0.2431 - val_acc: 0.5607\n",
      "Epoch 75/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2435 - acc: 0.5603 - val_loss: 0.2431 - val_acc: 0.5604\n",
      "Epoch 76/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2435 - acc: 0.5601 - val_loss: 0.2430 - val_acc: 0.5614\n",
      "Epoch 77/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5604 - val_loss: 0.2431 - val_acc: 0.5613\n",
      "Epoch 78/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2435 - acc: 0.5604 - val_loss: 0.2433 - val_acc: 0.5610\n",
      "Epoch 79/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2434 - acc: 0.5604 - val_loss: 0.2431 - val_acc: 0.5621\n",
      "Epoch 80/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2435 - acc: 0.5605 - val_loss: 0.2431 - val_acc: 0.5616\n",
      "Epoch 81/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5601 - val_loss: 0.2431 - val_acc: 0.5616\n",
      "Epoch 82/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5605 - val_loss: 0.2431 - val_acc: 0.5614\n",
      "Epoch 83/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2435 - acc: 0.5604 - val_loss: 0.2431 - val_acc: 0.5616\n",
      "Epoch 84/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2435 - acc: 0.5601 - val_loss: 0.2430 - val_acc: 0.5618\n",
      "Epoch 85/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2434 - acc: 0.5602 - val_loss: 0.2431 - val_acc: 0.5610\n",
      "Epoch 86/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2434 - acc: 0.5604 - val_loss: 0.2430 - val_acc: 0.5620\n",
      "Epoch 87/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2434 - acc: 0.5602 - val_loss: 0.2433 - val_acc: 0.5618\n",
      "Epoch 88/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5606 - val_loss: 0.2433 - val_acc: 0.5605\n",
      "Epoch 89/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5602 - val_loss: 0.2431 - val_acc: 0.5620\n",
      "Epoch 90/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2434 - acc: 0.5604 - val_loss: 0.2432 - val_acc: 0.5603\n",
      "Epoch 91/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5602 - val_loss: 0.2432 - val_acc: 0.5615\n",
      "Epoch 92/200\n",
      "1626098/1626098 [==============================] - 96s 59us/step - loss: 0.2434 - acc: 0.5605 - val_loss: 0.2431 - val_acc: 0.5621\n",
      "Epoch 93/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5600 - val_loss: 0.2430 - val_acc: 0.5619\n",
      "Epoch 94/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2435 - acc: 0.5602 - val_loss: 0.2430 - val_acc: 0.5615\n",
      "Epoch 95/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5606 - val_loss: 0.2433 - val_acc: 0.5607\n",
      "Epoch 96/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5605 - val_loss: 0.2434 - val_acc: 0.5615\n",
      "Epoch 97/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5610 - val_loss: 0.2432 - val_acc: 0.5610\n",
      "Epoch 98/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5602 - val_loss: 0.2433 - val_acc: 0.5611\n",
      "Epoch 99/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5605 - val_loss: 0.2431 - val_acc: 0.5620\n",
      "Epoch 100/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5603 - val_loss: 0.2431 - val_acc: 0.5620\n",
      "Epoch 101/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2434 - acc: 0.5608 - val_loss: 0.2432 - val_acc: 0.5618\n",
      "Epoch 102/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5606 - val_loss: 0.2433 - val_acc: 0.5617\n",
      "Epoch 103/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5605 - val_loss: 0.2431 - val_acc: 0.5617\n",
      "Epoch 104/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5609 - val_loss: 0.2431 - val_acc: 0.5617\n",
      "Epoch 105/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5605 - val_loss: 0.2434 - val_acc: 0.5614\n",
      "Epoch 106/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5608 - val_loss: 0.2432 - val_acc: 0.5613\n",
      "Epoch 107/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5607 - val_loss: 0.2431 - val_acc: 0.5605\n",
      "Epoch 108/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5608 - val_loss: 0.2431 - val_acc: 0.5616\n",
      "Epoch 109/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5606 - val_loss: 0.2432 - val_acc: 0.5612\n",
      "Epoch 110/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5605 - val_loss: 0.2430 - val_acc: 0.5614\n",
      "Epoch 111/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5605 - val_loss: 0.2433 - val_acc: 0.5600\n",
      "Epoch 112/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5606 - val_loss: 0.2430 - val_acc: 0.5617\n",
      "Epoch 113/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5605 - val_loss: 0.2432 - val_acc: 0.5613\n",
      "Epoch 114/200\n",
      "1626098/1626098 [==============================] - 96s 59us/step - loss: 0.2434 - acc: 0.5604 - val_loss: 0.2430 - val_acc: 0.5615\n",
      "Epoch 115/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5609 - val_loss: 0.2432 - val_acc: 0.5617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2434 - acc: 0.5607 - val_loss: 0.2431 - val_acc: 0.5618\n",
      "Epoch 117/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2433 - acc: 0.5604 - val_loss: 0.2432 - val_acc: 0.5601\n",
      "Epoch 118/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2434 - acc: 0.5605 - val_loss: 0.2430 - val_acc: 0.5618\n",
      "Epoch 119/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2434 - acc: 0.5602 - val_loss: 0.2431 - val_acc: 0.5615\n",
      "Epoch 120/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5609 - val_loss: 0.2430 - val_acc: 0.5616\n",
      "Epoch 121/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5604 - val_loss: 0.2432 - val_acc: 0.5618\n",
      "Epoch 122/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5606 - val_loss: 0.2432 - val_acc: 0.5616\n",
      "Epoch 123/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5608 - val_loss: 0.2432 - val_acc: 0.5615\n",
      "Epoch 124/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2433 - acc: 0.5603 - val_loss: 0.2432 - val_acc: 0.5607\n",
      "Epoch 125/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2434 - acc: 0.5605 - val_loss: 0.2431 - val_acc: 0.5617\n",
      "Epoch 126/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5610 - val_loss: 0.2430 - val_acc: 0.5618\n",
      "Epoch 127/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5607 - val_loss: 0.2431 - val_acc: 0.5616\n",
      "Epoch 128/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2433 - acc: 0.5608 - val_loss: 0.2431 - val_acc: 0.5619\n",
      "Epoch 129/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5609 - val_loss: 0.2429 - val_acc: 0.5619\n",
      "Epoch 130/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5608 - val_loss: 0.2430 - val_acc: 0.5619\n",
      "Epoch 131/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5606 - val_loss: 0.2431 - val_acc: 0.5613\n",
      "Epoch 132/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5608 - val_loss: 0.2430 - val_acc: 0.5614\n",
      "Epoch 133/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2433 - acc: 0.5607 - val_loss: 0.2430 - val_acc: 0.5616\n",
      "Epoch 134/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5610 - val_loss: 0.2431 - val_acc: 0.5619\n",
      "Epoch 135/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2433 - acc: 0.5610 - val_loss: 0.2430 - val_acc: 0.5614\n",
      "Epoch 136/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2433 - acc: 0.5608 - val_loss: 0.2430 - val_acc: 0.5609\n",
      "Epoch 137/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2434 - acc: 0.5607 - val_loss: 0.2431 - val_acc: 0.5613\n",
      "Epoch 138/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5608 - val_loss: 0.2432 - val_acc: 0.5607\n",
      "Epoch 139/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5607 - val_loss: 0.2432 - val_acc: 0.5614\n",
      "Epoch 140/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5609 - val_loss: 0.2431 - val_acc: 0.5609\n",
      "Epoch 141/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2433 - acc: 0.5605 - val_loss: 0.2432 - val_acc: 0.5600\n",
      "Epoch 142/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2433 - acc: 0.5608 - val_loss: 0.2430 - val_acc: 0.5618\n",
      "Epoch 143/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5607 - val_loss: 0.2429 - val_acc: 0.5618\n",
      "Epoch 144/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5608 - val_loss: 0.2431 - val_acc: 0.5614\n",
      "Epoch 145/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5611 - val_loss: 0.2430 - val_acc: 0.5614\n",
      "Epoch 146/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5601 - val_loss: 0.2429 - val_acc: 0.5617\n",
      "Epoch 147/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5606 - val_loss: 0.2433 - val_acc: 0.5617\n",
      "Epoch 148/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2434 - acc: 0.5610 - val_loss: 0.2430 - val_acc: 0.5615\n",
      "Epoch 149/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5609 - val_loss: 0.2432 - val_acc: 0.5603\n",
      "Epoch 150/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5609 - val_loss: 0.2431 - val_acc: 0.5617\n",
      "Epoch 151/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5604 - val_loss: 0.2430 - val_acc: 0.5620\n",
      "Epoch 152/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5605 - val_loss: 0.2430 - val_acc: 0.5619\n",
      "Epoch 153/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5609 - val_loss: 0.2430 - val_acc: 0.5617\n",
      "Epoch 154/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5609 - val_loss: 0.2432 - val_acc: 0.5607\n",
      "Epoch 155/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5611 - val_loss: 0.2431 - val_acc: 0.5617\n",
      "Epoch 156/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5609 - val_loss: 0.2431 - val_acc: 0.5614\n",
      "Epoch 157/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5606 - val_loss: 0.2430 - val_acc: 0.5618\n",
      "Epoch 158/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5609 - val_loss: 0.2430 - val_acc: 0.5612\n",
      "Epoch 159/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5607 - val_loss: 0.2433 - val_acc: 0.5606\n",
      "Epoch 160/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5609 - val_loss: 0.2433 - val_acc: 0.5615\n",
      "Epoch 161/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5611 - val_loss: 0.2432 - val_acc: 0.5611\n",
      "Epoch 162/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5608 - val_loss: 0.2430 - val_acc: 0.5608\n",
      "Epoch 163/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5608 - val_loss: 0.2432 - val_acc: 0.5612\n",
      "Epoch 164/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5610 - val_loss: 0.2430 - val_acc: 0.5617\n",
      "Epoch 165/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5609 - val_loss: 0.2432 - val_acc: 0.5616\n",
      "Epoch 166/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5609 - val_loss: 0.2430 - val_acc: 0.5611\n",
      "Epoch 167/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5609 - val_loss: 0.2432 - val_acc: 0.5616\n",
      "Epoch 168/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5610 - val_loss: 0.2431 - val_acc: 0.5611\n",
      "Epoch 169/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5612 - val_loss: 0.2432 - val_acc: 0.5612\n",
      "Epoch 170/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5607 - val_loss: 0.2430 - val_acc: 0.5612\n",
      "Epoch 171/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5605 - val_loss: 0.2429 - val_acc: 0.5619\n",
      "Epoch 172/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2433 - acc: 0.5606 - val_loss: 0.2430 - val_acc: 0.5616\n",
      "Epoch 173/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2433 - acc: 0.5610 - val_loss: 0.2432 - val_acc: 0.5617\n",
      "Epoch 174/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2433 - acc: 0.5608 - val_loss: 0.2430 - val_acc: 0.5616\n",
      "Epoch 175/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2433 - acc: 0.5608 - val_loss: 0.2430 - val_acc: 0.5617\n",
      "Epoch 176/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2433 - acc: 0.5608 - val_loss: 0.2431 - val_acc: 0.5616\n",
      "Epoch 177/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2432 - acc: 0.5615 - val_loss: 0.2432 - val_acc: 0.5616\n",
      "Epoch 178/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2433 - acc: 0.5609 - val_loss: 0.2430 - val_acc: 0.5621\n",
      "Epoch 179/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2433 - acc: 0.5611 - val_loss: 0.2432 - val_acc: 0.5603\n",
      "Epoch 180/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2433 - acc: 0.5610 - val_loss: 0.2431 - val_acc: 0.5618\n",
      "Epoch 181/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2433 - acc: 0.5607 - val_loss: 0.2430 - val_acc: 0.5608\n",
      "Epoch 182/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2433 - acc: 0.5609 - val_loss: 0.2432 - val_acc: 0.5613\n",
      "Epoch 183/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2433 - acc: 0.5610 - val_loss: 0.2432 - val_acc: 0.5604\n",
      "Epoch 184/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5606 - val_loss: 0.2430 - val_acc: 0.5614\n",
      "Epoch 185/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5612 - val_loss: 0.2431 - val_acc: 0.5616\n",
      "Epoch 186/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5607 - val_loss: 0.2431 - val_acc: 0.5611\n",
      "Epoch 187/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5608 - val_loss: 0.2431 - val_acc: 0.5618\n",
      "Epoch 188/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2432 - acc: 0.5608 - val_loss: 0.2430 - val_acc: 0.5611\n",
      "Epoch 189/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5608 - val_loss: 0.2432 - val_acc: 0.5613\n",
      "Epoch 190/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5607 - val_loss: 0.2430 - val_acc: 0.5613\n",
      "Epoch 191/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2432 - acc: 0.5608 - val_loss: 0.2430 - val_acc: 0.5614\n",
      "Epoch 192/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2433 - acc: 0.5608 - val_loss: 0.2430 - val_acc: 0.5618\n",
      "Epoch 193/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5608 - val_loss: 0.2433 - val_acc: 0.5612\n",
      "Epoch 194/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2433 - acc: 0.5607 - val_loss: 0.2431 - val_acc: 0.5606\n",
      "Epoch 195/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2432 - acc: 0.5610 - val_loss: 0.2432 - val_acc: 0.5611\n",
      "Epoch 196/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5605 - val_loss: 0.2430 - val_acc: 0.5618\n",
      "Epoch 197/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2432 - acc: 0.5610 - val_loss: 0.2430 - val_acc: 0.5616\n",
      "Epoch 198/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2432 - acc: 0.5610 - val_loss: 0.2430 - val_acc: 0.5612\n",
      "Epoch 199/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5609 - val_loss: 0.2434 - val_acc: 0.5610\n",
      "Epoch 200/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2433 - acc: 0.5607 - val_loss: 0.2430 - val_acc: 0.5615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd494a8be80>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7 Layers\n",
    "model = Sequential()\n",
    "model.add(Dense(250, activation='relu', input_shape=(X.shape[1],)))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer=adam, loss='mean_squared_error', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=200, validation_split=0.3, callbacks=[early_stop], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred > 0.5\n",
    "accuracy_plot(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win rate: 56.01133936653142%\n"
     ]
    }
   ],
   "source": [
    "print('win rate: {}%'.format(accuracy_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('h5/keras_data3_M1_56.h5')\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('keras_53.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1626098 samples, validate on 696900 samples\n",
      "Epoch 1/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2452 - acc: 0.5554 - val_loss: 0.2442 - val_acc: 0.5584\n",
      "Epoch 2/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2441 - acc: 0.5582 - val_loss: 0.2436 - val_acc: 0.5597\n",
      "Epoch 3/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2438 - acc: 0.5593 - val_loss: 0.2437 - val_acc: 0.5594\n",
      "Epoch 4/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2436 - acc: 0.5598 - val_loss: 0.2434 - val_acc: 0.5602\n",
      "Epoch 5/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2434 - acc: 0.5603 - val_loss: 0.2434 - val_acc: 0.5609\n",
      "Epoch 6/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2432 - acc: 0.5610 - val_loss: 0.2433 - val_acc: 0.5610\n",
      "Epoch 7/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2431 - acc: 0.5613 - val_loss: 0.2431 - val_acc: 0.5610\n",
      "Epoch 8/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2429 - acc: 0.5618 - val_loss: 0.2432 - val_acc: 0.5608\n",
      "Epoch 9/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2428 - acc: 0.5623 - val_loss: 0.2432 - val_acc: 0.5615\n",
      "Epoch 10/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2427 - acc: 0.5625 - val_loss: 0.2432 - val_acc: 0.5604\n",
      "Epoch 11/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2426 - acc: 0.5632 - val_loss: 0.2432 - val_acc: 0.5614\n",
      "Epoch 12/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2425 - acc: 0.5636 - val_loss: 0.2432 - val_acc: 0.5611\n",
      "Epoch 13/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2423 - acc: 0.5639 - val_loss: 0.2432 - val_acc: 0.5605\n",
      "Epoch 14/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2422 - acc: 0.5641 - val_loss: 0.2432 - val_acc: 0.5615\n",
      "Epoch 15/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2421 - acc: 0.5646 - val_loss: 0.2433 - val_acc: 0.5603\n",
      "Epoch 16/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2420 - acc: 0.5654 - val_loss: 0.2433 - val_acc: 0.5603\n",
      "Epoch 17/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2419 - acc: 0.5659 - val_loss: 0.2434 - val_acc: 0.5604\n",
      "Epoch 18/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2418 - acc: 0.5661 - val_loss: 0.2433 - val_acc: 0.5602\n",
      "Epoch 19/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2417 - acc: 0.5666 - val_loss: 0.2434 - val_acc: 0.5603\n",
      "Epoch 20/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2415 - acc: 0.5673 - val_loss: 0.2436 - val_acc: 0.5604\n",
      "Epoch 21/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2414 - acc: 0.5672 - val_loss: 0.2435 - val_acc: 0.5599\n",
      "Epoch 22/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2413 - acc: 0.5679 - val_loss: 0.2437 - val_acc: 0.5605\n",
      "Epoch 23/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2412 - acc: 0.5686 - val_loss: 0.2435 - val_acc: 0.5595\n",
      "Epoch 24/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2410 - acc: 0.5688 - val_loss: 0.2438 - val_acc: 0.5598\n",
      "Epoch 25/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2409 - acc: 0.5693 - val_loss: 0.2437 - val_acc: 0.5597\n",
      "Epoch 26/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2407 - acc: 0.5698 - val_loss: 0.2438 - val_acc: 0.5594\n",
      "Epoch 27/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2406 - acc: 0.5704 - val_loss: 0.2439 - val_acc: 0.5593\n",
      "Epoch 28/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2405 - acc: 0.5709 - val_loss: 0.2440 - val_acc: 0.5589\n",
      "Epoch 29/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2403 - acc: 0.5711 - val_loss: 0.2443 - val_acc: 0.5587\n",
      "Epoch 30/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2402 - acc: 0.5715 - val_loss: 0.2441 - val_acc: 0.5593\n",
      "Epoch 31/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2401 - acc: 0.5724 - val_loss: 0.2443 - val_acc: 0.5583\n",
      "Epoch 32/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2400 - acc: 0.5727 - val_loss: 0.2443 - val_acc: 0.5586\n",
      "Epoch 33/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2398 - acc: 0.5733 - val_loss: 0.2446 - val_acc: 0.5584\n",
      "Epoch 34/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2397 - acc: 0.5738 - val_loss: 0.2448 - val_acc: 0.5574\n",
      "Epoch 35/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2395 - acc: 0.5744 - val_loss: 0.2446 - val_acc: 0.5579\n",
      "Epoch 36/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2394 - acc: 0.5750 - val_loss: 0.2453 - val_acc: 0.5570\n",
      "Epoch 37/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2393 - acc: 0.5749 - val_loss: 0.2447 - val_acc: 0.5580\n",
      "Epoch 38/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2391 - acc: 0.5756 - val_loss: 0.2451 - val_acc: 0.5581\n",
      "Epoch 39/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2390 - acc: 0.5761 - val_loss: 0.2452 - val_acc: 0.5578\n",
      "Epoch 40/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2388 - acc: 0.5767 - val_loss: 0.2453 - val_acc: 0.5562\n",
      "Epoch 41/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2387 - acc: 0.5770 - val_loss: 0.2453 - val_acc: 0.5568\n",
      "Epoch 42/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2385 - acc: 0.5771 - val_loss: 0.2458 - val_acc: 0.5572\n",
      "Epoch 43/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2384 - acc: 0.5779 - val_loss: 0.2459 - val_acc: 0.5571\n",
      "Epoch 44/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2382 - acc: 0.5787 - val_loss: 0.2456 - val_acc: 0.5563\n",
      "Epoch 45/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2381 - acc: 0.5791 - val_loss: 0.2456 - val_acc: 0.5561\n",
      "Epoch 46/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2380 - acc: 0.5794 - val_loss: 0.2462 - val_acc: 0.5571\n",
      "Epoch 47/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2379 - acc: 0.5801 - val_loss: 0.2461 - val_acc: 0.5548\n",
      "Epoch 48/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2378 - acc: 0.5802 - val_loss: 0.2458 - val_acc: 0.5551\n",
      "Epoch 49/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2377 - acc: 0.5803 - val_loss: 0.2461 - val_acc: 0.5546\n",
      "Epoch 50/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2375 - acc: 0.5810 - val_loss: 0.2467 - val_acc: 0.5550\n",
      "Epoch 51/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2374 - acc: 0.5813 - val_loss: 0.2471 - val_acc: 0.5553\n",
      "Epoch 52/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2373 - acc: 0.5818 - val_loss: 0.2467 - val_acc: 0.5570\n",
      "Epoch 53/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2371 - acc: 0.5820 - val_loss: 0.2468 - val_acc: 0.5543\n",
      "Epoch 54/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2370 - acc: 0.5825 - val_loss: 0.2465 - val_acc: 0.5552\n",
      "Epoch 55/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2369 - acc: 0.5831 - val_loss: 0.2474 - val_acc: 0.5551\n",
      "Epoch 56/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2368 - acc: 0.5833 - val_loss: 0.2472 - val_acc: 0.5543\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2367 - acc: 0.5833 - val_loss: 0.2467 - val_acc: 0.5553\n",
      "Epoch 58/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2366 - acc: 0.5841 - val_loss: 0.2473 - val_acc: 0.5552\n",
      "Epoch 59/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2365 - acc: 0.5842 - val_loss: 0.2476 - val_acc: 0.5536\n",
      "Epoch 60/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2364 - acc: 0.5843 - val_loss: 0.2475 - val_acc: 0.5530\n",
      "Epoch 61/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2363 - acc: 0.5850 - val_loss: 0.2473 - val_acc: 0.5526\n",
      "Epoch 62/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2361 - acc: 0.5850 - val_loss: 0.2479 - val_acc: 0.5540\n",
      "Epoch 63/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2360 - acc: 0.5856 - val_loss: 0.2476 - val_acc: 0.5539\n",
      "Epoch 64/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2359 - acc: 0.5862 - val_loss: 0.2484 - val_acc: 0.5527\n",
      "Epoch 65/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2358 - acc: 0.5859 - val_loss: 0.2475 - val_acc: 0.5531\n",
      "Epoch 66/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2357 - acc: 0.5866 - val_loss: 0.2481 - val_acc: 0.5537\n",
      "Epoch 67/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2355 - acc: 0.5871 - val_loss: 0.2479 - val_acc: 0.5529\n",
      "Epoch 68/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2355 - acc: 0.5875 - val_loss: 0.2483 - val_acc: 0.5531\n",
      "Epoch 69/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2354 - acc: 0.5878 - val_loss: 0.2480 - val_acc: 0.5524\n",
      "Epoch 70/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2353 - acc: 0.5877 - val_loss: 0.2481 - val_acc: 0.5531\n",
      "Epoch 71/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2352 - acc: 0.5883 - val_loss: 0.2485 - val_acc: 0.5527\n",
      "Epoch 72/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2351 - acc: 0.5888 - val_loss: 0.2484 - val_acc: 0.5522\n",
      "Epoch 73/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2350 - acc: 0.5888 - val_loss: 0.2482 - val_acc: 0.5512\n",
      "Epoch 74/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2349 - acc: 0.5888 - val_loss: 0.2486 - val_acc: 0.5532\n",
      "Epoch 75/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2348 - acc: 0.5890 - val_loss: 0.2484 - val_acc: 0.5527\n",
      "Epoch 76/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2347 - acc: 0.5893 - val_loss: 0.2487 - val_acc: 0.5524\n",
      "Epoch 77/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2346 - acc: 0.5896 - val_loss: 0.2489 - val_acc: 0.5518\n",
      "Epoch 78/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2345 - acc: 0.5899 - val_loss: 0.2499 - val_acc: 0.5531\n",
      "Epoch 79/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2345 - acc: 0.5903 - val_loss: 0.2491 - val_acc: 0.5521\n",
      "Epoch 80/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2344 - acc: 0.5903 - val_loss: 0.2494 - val_acc: 0.5525\n",
      "Epoch 81/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2343 - acc: 0.5909 - val_loss: 0.2497 - val_acc: 0.5529\n",
      "Epoch 82/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2342 - acc: 0.5913 - val_loss: 0.2491 - val_acc: 0.5516\n",
      "Epoch 83/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2341 - acc: 0.5915 - val_loss: 0.2493 - val_acc: 0.5511\n",
      "Epoch 84/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2340 - acc: 0.5915 - val_loss: 0.2491 - val_acc: 0.5509\n",
      "Epoch 85/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2339 - acc: 0.5920 - val_loss: 0.2499 - val_acc: 0.5500\n",
      "Epoch 86/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2339 - acc: 0.5925 - val_loss: 0.2498 - val_acc: 0.5502\n",
      "Epoch 87/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2337 - acc: 0.5929 - val_loss: 0.2492 - val_acc: 0.5514\n",
      "Epoch 88/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2336 - acc: 0.5926 - val_loss: 0.2503 - val_acc: 0.5508\n",
      "Epoch 89/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2336 - acc: 0.5929 - val_loss: 0.2491 - val_acc: 0.5502\n",
      "Epoch 90/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2335 - acc: 0.5930 - val_loss: 0.2513 - val_acc: 0.5499\n",
      "Epoch 91/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2335 - acc: 0.5931 - val_loss: 0.2504 - val_acc: 0.5502\n",
      "Epoch 92/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2334 - acc: 0.5933 - val_loss: 0.2499 - val_acc: 0.5515\n",
      "Epoch 93/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2333 - acc: 0.5937 - val_loss: 0.2505 - val_acc: 0.5512\n",
      "Epoch 94/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2332 - acc: 0.5941 - val_loss: 0.2502 - val_acc: 0.5510\n",
      "Epoch 95/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2332 - acc: 0.5940 - val_loss: 0.2509 - val_acc: 0.5510\n",
      "Epoch 96/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2331 - acc: 0.5942 - val_loss: 0.2509 - val_acc: 0.5515\n",
      "Epoch 97/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2331 - acc: 0.5945 - val_loss: 0.2505 - val_acc: 0.5509\n",
      "Epoch 98/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2329 - acc: 0.5949 - val_loss: 0.2508 - val_acc: 0.5509\n",
      "Epoch 99/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2329 - acc: 0.5949 - val_loss: 0.2505 - val_acc: 0.5509\n",
      "Epoch 100/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2328 - acc: 0.5948 - val_loss: 0.2513 - val_acc: 0.5500\n",
      "Epoch 101/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2328 - acc: 0.5952 - val_loss: 0.2497 - val_acc: 0.5500\n",
      "Epoch 102/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2327 - acc: 0.5957 - val_loss: 0.2517 - val_acc: 0.5507\n",
      "Epoch 103/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2327 - acc: 0.5956 - val_loss: 0.2511 - val_acc: 0.5499\n",
      "Epoch 104/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2325 - acc: 0.5956 - val_loss: 0.2514 - val_acc: 0.5506\n",
      "Epoch 105/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2325 - acc: 0.5959 - val_loss: 0.2506 - val_acc: 0.5501\n",
      "Epoch 106/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2324 - acc: 0.5957 - val_loss: 0.2514 - val_acc: 0.5509\n",
      "Epoch 107/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2324 - acc: 0.5961 - val_loss: 0.2518 - val_acc: 0.5502\n",
      "Epoch 108/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2323 - acc: 0.5966 - val_loss: 0.2522 - val_acc: 0.5502\n",
      "Epoch 109/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2322 - acc: 0.5964 - val_loss: 0.2509 - val_acc: 0.5503\n",
      "Epoch 110/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2322 - acc: 0.5969 - val_loss: 0.2510 - val_acc: 0.5493\n",
      "Epoch 111/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2322 - acc: 0.5964 - val_loss: 0.2518 - val_acc: 0.5502\n",
      "Epoch 112/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2321 - acc: 0.5972 - val_loss: 0.2518 - val_acc: 0.5492\n",
      "Epoch 113/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2321 - acc: 0.5969 - val_loss: 0.2522 - val_acc: 0.5517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2319 - acc: 0.5978 - val_loss: 0.2519 - val_acc: 0.5502\n",
      "Epoch 115/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2319 - acc: 0.5971 - val_loss: 0.2522 - val_acc: 0.5482\n",
      "Epoch 116/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2319 - acc: 0.5973 - val_loss: 0.2522 - val_acc: 0.5487\n",
      "Epoch 117/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2319 - acc: 0.5972 - val_loss: 0.2512 - val_acc: 0.5494\n",
      "Epoch 118/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2318 - acc: 0.5978 - val_loss: 0.2521 - val_acc: 0.5486\n",
      "Epoch 119/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2318 - acc: 0.5978 - val_loss: 0.2519 - val_acc: 0.5491\n",
      "Epoch 120/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2317 - acc: 0.5982 - val_loss: 0.2526 - val_acc: 0.5475\n",
      "Epoch 121/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2317 - acc: 0.5980 - val_loss: 0.2525 - val_acc: 0.5502\n",
      "Epoch 122/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2316 - acc: 0.5980 - val_loss: 0.2520 - val_acc: 0.5494\n",
      "Epoch 123/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2315 - acc: 0.5984 - val_loss: 0.2526 - val_acc: 0.5491\n",
      "Epoch 124/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2315 - acc: 0.5983 - val_loss: 0.2523 - val_acc: 0.5505\n",
      "Epoch 125/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2314 - acc: 0.5987 - val_loss: 0.2526 - val_acc: 0.5492\n",
      "Epoch 126/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2315 - acc: 0.5984 - val_loss: 0.2524 - val_acc: 0.5489\n",
      "Epoch 127/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2314 - acc: 0.5990 - val_loss: 0.2520 - val_acc: 0.5492\n",
      "Epoch 128/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2313 - acc: 0.5992 - val_loss: 0.2525 - val_acc: 0.5488\n",
      "Epoch 129/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2312 - acc: 0.5989 - val_loss: 0.2527 - val_acc: 0.5493\n",
      "Epoch 130/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2312 - acc: 0.5990 - val_loss: 0.2531 - val_acc: 0.5491\n",
      "Epoch 131/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2311 - acc: 0.5994 - val_loss: 0.2521 - val_acc: 0.5487\n",
      "Epoch 132/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2311 - acc: 0.5995 - val_loss: 0.2531 - val_acc: 0.5494\n",
      "Epoch 133/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2311 - acc: 0.5995 - val_loss: 0.2518 - val_acc: 0.5503\n",
      "Epoch 134/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2311 - acc: 0.6001 - val_loss: 0.2525 - val_acc: 0.5482\n",
      "Epoch 135/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2310 - acc: 0.5998 - val_loss: 0.2531 - val_acc: 0.5493\n",
      "Epoch 136/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2309 - acc: 0.6003 - val_loss: 0.2532 - val_acc: 0.5491\n",
      "Epoch 137/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2309 - acc: 0.6000 - val_loss: 0.2531 - val_acc: 0.5489\n",
      "Epoch 138/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2309 - acc: 0.6000 - val_loss: 0.2527 - val_acc: 0.5484\n",
      "Epoch 139/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2308 - acc: 0.6003 - val_loss: 0.2525 - val_acc: 0.5480\n",
      "Epoch 140/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2308 - acc: 0.6002 - val_loss: 0.2523 - val_acc: 0.5478\n",
      "Epoch 141/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2308 - acc: 0.6004 - val_loss: 0.2539 - val_acc: 0.5478\n",
      "Epoch 142/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2308 - acc: 0.6005 - val_loss: 0.2530 - val_acc: 0.5487\n",
      "Epoch 143/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2307 - acc: 0.6007 - val_loss: 0.2527 - val_acc: 0.5482\n",
      "Epoch 144/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2307 - acc: 0.6005 - val_loss: 0.2524 - val_acc: 0.5483\n",
      "Epoch 145/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2306 - acc: 0.6006 - val_loss: 0.2528 - val_acc: 0.5489\n",
      "Epoch 146/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2306 - acc: 0.6010 - val_loss: 0.2541 - val_acc: 0.5486\n",
      "Epoch 147/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2306 - acc: 0.6010 - val_loss: 0.2537 - val_acc: 0.5489\n",
      "Epoch 148/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2305 - acc: 0.6011 - val_loss: 0.2529 - val_acc: 0.5476\n",
      "Epoch 149/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2306 - acc: 0.6010 - val_loss: 0.2536 - val_acc: 0.5496\n",
      "Epoch 150/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2304 - acc: 0.6013 - val_loss: 0.2536 - val_acc: 0.5488\n",
      "Epoch 151/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2304 - acc: 0.6015 - val_loss: 0.2528 - val_acc: 0.5479\n",
      "Epoch 152/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2303 - acc: 0.6019 - val_loss: 0.2532 - val_acc: 0.5478\n",
      "Epoch 153/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2304 - acc: 0.6013 - val_loss: 0.2533 - val_acc: 0.5477\n",
      "Epoch 154/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2304 - acc: 0.6016 - val_loss: 0.2535 - val_acc: 0.5476\n",
      "Epoch 155/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2302 - acc: 0.6019 - val_loss: 0.2539 - val_acc: 0.5488\n",
      "Epoch 156/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2302 - acc: 0.6014 - val_loss: 0.2544 - val_acc: 0.5480\n",
      "Epoch 157/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2303 - acc: 0.6016 - val_loss: 0.2539 - val_acc: 0.5495\n",
      "Epoch 158/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2302 - acc: 0.6019 - val_loss: 0.2534 - val_acc: 0.5489\n",
      "Epoch 159/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2301 - acc: 0.6022 - val_loss: 0.2537 - val_acc: 0.5477\n",
      "Epoch 160/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2302 - acc: 0.6016 - val_loss: 0.2534 - val_acc: 0.5484\n",
      "Epoch 161/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2301 - acc: 0.6020 - val_loss: 0.2543 - val_acc: 0.5478\n",
      "Epoch 162/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2302 - acc: 0.6018 - val_loss: 0.2534 - val_acc: 0.5483\n",
      "Epoch 163/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2300 - acc: 0.6025 - val_loss: 0.2544 - val_acc: 0.5485\n",
      "Epoch 164/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2300 - acc: 0.6023 - val_loss: 0.2538 - val_acc: 0.5479\n",
      "Epoch 165/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2299 - acc: 0.6020 - val_loss: 0.2529 - val_acc: 0.5475\n",
      "Epoch 166/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2299 - acc: 0.6020 - val_loss: 0.2543 - val_acc: 0.5475\n",
      "Epoch 167/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2299 - acc: 0.6029 - val_loss: 0.2539 - val_acc: 0.5493\n",
      "Epoch 168/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2299 - acc: 0.6027 - val_loss: 0.2550 - val_acc: 0.5495\n",
      "Epoch 169/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2298 - acc: 0.6025 - val_loss: 0.2542 - val_acc: 0.5487\n",
      "Epoch 170/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2298 - acc: 0.6028 - val_loss: 0.2549 - val_acc: 0.5489\n",
      "Epoch 171/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2298 - acc: 0.6031 - val_loss: 0.2548 - val_acc: 0.5472\n",
      "Epoch 172/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2298 - acc: 0.6031 - val_loss: 0.2543 - val_acc: 0.5490\n",
      "Epoch 173/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2297 - acc: 0.6026 - val_loss: 0.2543 - val_acc: 0.5492\n",
      "Epoch 174/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2298 - acc: 0.6028 - val_loss: 0.2548 - val_acc: 0.5485\n",
      "Epoch 175/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2297 - acc: 0.6025 - val_loss: 0.2548 - val_acc: 0.5488\n",
      "Epoch 176/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2298 - acc: 0.6024 - val_loss: 0.2538 - val_acc: 0.5478\n",
      "Epoch 177/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2296 - acc: 0.6030 - val_loss: 0.2539 - val_acc: 0.5477\n",
      "Epoch 178/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2296 - acc: 0.6030 - val_loss: 0.2551 - val_acc: 0.5478\n",
      "Epoch 179/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2296 - acc: 0.6032 - val_loss: 0.2553 - val_acc: 0.5482\n",
      "Epoch 180/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2296 - acc: 0.6034 - val_loss: 0.2539 - val_acc: 0.5474\n",
      "Epoch 181/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2295 - acc: 0.6031 - val_loss: 0.2548 - val_acc: 0.5482\n",
      "Epoch 182/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2295 - acc: 0.6034 - val_loss: 0.2538 - val_acc: 0.5486\n",
      "Epoch 183/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2294 - acc: 0.6035 - val_loss: 0.2545 - val_acc: 0.5468\n",
      "Epoch 184/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2295 - acc: 0.6033 - val_loss: 0.2553 - val_acc: 0.5478\n",
      "Epoch 185/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2295 - acc: 0.6032 - val_loss: 0.2547 - val_acc: 0.5486\n",
      "Epoch 186/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2295 - acc: 0.6034 - val_loss: 0.2537 - val_acc: 0.5489\n",
      "Epoch 187/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2295 - acc: 0.6036 - val_loss: 0.2543 - val_acc: 0.5482\n",
      "Epoch 188/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2294 - acc: 0.6035 - val_loss: 0.2544 - val_acc: 0.5484\n",
      "Epoch 189/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2294 - acc: 0.6035 - val_loss: 0.2548 - val_acc: 0.5471\n",
      "Epoch 190/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2293 - acc: 0.6040 - val_loss: 0.2539 - val_acc: 0.5481\n",
      "Epoch 191/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2293 - acc: 0.6034 - val_loss: 0.2553 - val_acc: 0.5480\n",
      "Epoch 192/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2293 - acc: 0.6040 - val_loss: 0.2542 - val_acc: 0.5490\n",
      "Epoch 193/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2294 - acc: 0.6034 - val_loss: 0.2538 - val_acc: 0.5491\n",
      "Epoch 194/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2293 - acc: 0.6041 - val_loss: 0.2542 - val_acc: 0.5481\n",
      "Epoch 195/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2292 - acc: 0.6040 - val_loss: 0.2553 - val_acc: 0.5475\n",
      "Epoch 196/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2292 - acc: 0.6043 - val_loss: 0.2555 - val_acc: 0.5485\n",
      "Epoch 197/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2293 - acc: 0.6038 - val_loss: 0.2551 - val_acc: 0.5485\n",
      "Epoch 198/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2292 - acc: 0.6043 - val_loss: 0.2549 - val_acc: 0.5475\n",
      "Epoch 199/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2292 - acc: 0.6041 - val_loss: 0.2546 - val_acc: 0.5465\n",
      "Epoch 200/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2292 - acc: 0.6044 - val_loss: 0.2552 - val_acc: 0.5484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd47b5f1e10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(250, activation='relu', input_shape=(X.shape[1],)))\n",
    "model2.add(Dense(250, activation='relu'))\n",
    "model2.add(Dense(250, activation='relu'))\n",
    "model2.add(Dropout(.5))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model2.compile(optimizer=adam, loss='mean_squared_error', metrics=['accuracy'])\n",
    "model2.fit(X_train, y_train, epochs=200, validation_split=0.3, callbacks=[early_stop], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1626098 samples, validate on 696900 samples\n",
      "Epoch 1/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2449 - acc: 0.5561 - val_loss: 0.2438 - val_acc: 0.5592\n",
      "Epoch 2/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2439 - acc: 0.5592 - val_loss: 0.2438 - val_acc: 0.5595\n",
      "Epoch 3/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2435 - acc: 0.5599 - val_loss: 0.2436 - val_acc: 0.5597\n",
      "Epoch 4/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2433 - acc: 0.5609 - val_loss: 0.2434 - val_acc: 0.5601\n",
      "Epoch 5/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2431 - acc: 0.5615 - val_loss: 0.2432 - val_acc: 0.5611\n",
      "Epoch 6/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2429 - acc: 0.5618 - val_loss: 0.2433 - val_acc: 0.5606\n",
      "Epoch 7/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2428 - acc: 0.5627 - val_loss: 0.2433 - val_acc: 0.5608\n",
      "Epoch 8/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2426 - acc: 0.5633 - val_loss: 0.2431 - val_acc: 0.5611\n",
      "Epoch 9/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2424 - acc: 0.5638 - val_loss: 0.2431 - val_acc: 0.5614\n",
      "Epoch 10/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2422 - acc: 0.5648 - val_loss: 0.2431 - val_acc: 0.5611\n",
      "Epoch 11/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2421 - acc: 0.5652 - val_loss: 0.2432 - val_acc: 0.5612\n",
      "Epoch 12/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2419 - acc: 0.5662 - val_loss: 0.2433 - val_acc: 0.5603\n",
      "Epoch 13/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2417 - acc: 0.5668 - val_loss: 0.2434 - val_acc: 0.5612\n",
      "Epoch 14/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2415 - acc: 0.5678 - val_loss: 0.2436 - val_acc: 0.5597\n",
      "Epoch 15/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2413 - acc: 0.5687 - val_loss: 0.2435 - val_acc: 0.5598\n",
      "Epoch 16/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2411 - acc: 0.5695 - val_loss: 0.2437 - val_acc: 0.5594\n",
      "Epoch 17/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2408 - acc: 0.5709 - val_loss: 0.2438 - val_acc: 0.5598\n",
      "Epoch 18/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2406 - acc: 0.5717 - val_loss: 0.2441 - val_acc: 0.5578\n",
      "Epoch 19/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2403 - acc: 0.5732 - val_loss: 0.2443 - val_acc: 0.5577\n",
      "Epoch 20/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2401 - acc: 0.5743 - val_loss: 0.2445 - val_acc: 0.5578\n",
      "Epoch 21/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2398 - acc: 0.5754 - val_loss: 0.2443 - val_acc: 0.5575\n",
      "Epoch 22/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2395 - acc: 0.5767 - val_loss: 0.2448 - val_acc: 0.5556\n",
      "Epoch 23/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2392 - acc: 0.5782 - val_loss: 0.2453 - val_acc: 0.5554\n",
      "Epoch 24/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2389 - acc: 0.5790 - val_loss: 0.2454 - val_acc: 0.5538\n",
      "Epoch 25/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2385 - acc: 0.5804 - val_loss: 0.2456 - val_acc: 0.5537\n",
      "Epoch 26/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2381 - acc: 0.5821 - val_loss: 0.2460 - val_acc: 0.5520\n",
      "Epoch 27/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2378 - acc: 0.5836 - val_loss: 0.2460 - val_acc: 0.5539\n",
      "Epoch 28/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2375 - acc: 0.5846 - val_loss: 0.2465 - val_acc: 0.5533\n",
      "Epoch 29/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2371 - acc: 0.5860 - val_loss: 0.2467 - val_acc: 0.5514\n",
      "Epoch 30/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2367 - acc: 0.5875 - val_loss: 0.2473 - val_acc: 0.5488\n",
      "Epoch 31/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2363 - acc: 0.5891 - val_loss: 0.2478 - val_acc: 0.5485\n",
      "Epoch 32/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2359 - acc: 0.5902 - val_loss: 0.2478 - val_acc: 0.5531\n",
      "Epoch 33/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2356 - acc: 0.5918 - val_loss: 0.2484 - val_acc: 0.5496\n",
      "Epoch 34/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2352 - acc: 0.5927 - val_loss: 0.2487 - val_acc: 0.5483\n",
      "Epoch 35/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2348 - acc: 0.5942 - val_loss: 0.2488 - val_acc: 0.5495\n",
      "Epoch 36/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2344 - acc: 0.5959 - val_loss: 0.2492 - val_acc: 0.5508\n",
      "Epoch 37/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2340 - acc: 0.5972 - val_loss: 0.2493 - val_acc: 0.5490\n",
      "Epoch 38/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2337 - acc: 0.5981 - val_loss: 0.2500 - val_acc: 0.5482\n",
      "Epoch 39/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2333 - acc: 0.5994 - val_loss: 0.2504 - val_acc: 0.5469\n",
      "Epoch 40/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2330 - acc: 0.6005 - val_loss: 0.2509 - val_acc: 0.5469\n",
      "Epoch 41/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2326 - acc: 0.6015 - val_loss: 0.2518 - val_acc: 0.5449\n",
      "Epoch 42/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2322 - acc: 0.6029 - val_loss: 0.2516 - val_acc: 0.5454\n",
      "Epoch 43/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2319 - acc: 0.6039 - val_loss: 0.2521 - val_acc: 0.5438\n",
      "Epoch 44/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2315 - acc: 0.6049 - val_loss: 0.2521 - val_acc: 0.5452\n",
      "Epoch 45/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2312 - acc: 0.6060 - val_loss: 0.2525 - val_acc: 0.5448\n",
      "Epoch 46/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2308 - acc: 0.6075 - val_loss: 0.2531 - val_acc: 0.5452\n",
      "Epoch 47/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2305 - acc: 0.6085 - val_loss: 0.2528 - val_acc: 0.5440\n",
      "Epoch 48/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2302 - acc: 0.6093 - val_loss: 0.2540 - val_acc: 0.5436\n",
      "Epoch 49/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2299 - acc: 0.6107 - val_loss: 0.2540 - val_acc: 0.5431\n",
      "Epoch 50/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2295 - acc: 0.6114 - val_loss: 0.2541 - val_acc: 0.5428\n",
      "Epoch 51/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2292 - acc: 0.6128 - val_loss: 0.2546 - val_acc: 0.5437\n",
      "Epoch 52/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2290 - acc: 0.6130 - val_loss: 0.2550 - val_acc: 0.5438\n",
      "Epoch 53/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2286 - acc: 0.6140 - val_loss: 0.2553 - val_acc: 0.5415\n",
      "Epoch 54/200\n",
      "1626098/1626098 [==============================] - 94s 58us/step - loss: 0.2283 - acc: 0.6150 - val_loss: 0.2552 - val_acc: 0.5419\n",
      "Epoch 55/200\n",
      "1626098/1626098 [==============================] - 93s 57us/step - loss: 0.2280 - acc: 0.6161 - val_loss: 0.2560 - val_acc: 0.5398\n",
      "Epoch 56/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2277 - acc: 0.6167 - val_loss: 0.2565 - val_acc: 0.5411\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2275 - acc: 0.6175 - val_loss: 0.2565 - val_acc: 0.5432\n",
      "Epoch 58/200\n",
      "1626098/1626098 [==============================] - 99s 61us/step - loss: 0.2272 - acc: 0.6184 - val_loss: 0.2572 - val_acc: 0.5403\n",
      "Epoch 59/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2269 - acc: 0.6195 - val_loss: 0.2576 - val_acc: 0.5394\n",
      "Epoch 60/200\n",
      "1626098/1626098 [==============================] - 96s 59us/step - loss: 0.2267 - acc: 0.6197 - val_loss: 0.2571 - val_acc: 0.5434\n",
      "Epoch 61/200\n",
      "1626098/1626098 [==============================] - 87s 54us/step - loss: 0.2264 - acc: 0.6206 - val_loss: 0.2580 - val_acc: 0.5417\n",
      "Epoch 62/200\n",
      "1626098/1626098 [==============================] - 90s 56us/step - loss: 0.2261 - acc: 0.6219 - val_loss: 0.2585 - val_acc: 0.5390\n",
      "Epoch 63/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2259 - acc: 0.6224 - val_loss: 0.2585 - val_acc: 0.5423\n",
      "Epoch 64/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2256 - acc: 0.6230 - val_loss: 0.2589 - val_acc: 0.5406\n",
      "Epoch 65/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2253 - acc: 0.6240 - val_loss: 0.2594 - val_acc: 0.5376\n",
      "Epoch 66/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2251 - acc: 0.6246 - val_loss: 0.2597 - val_acc: 0.5408\n",
      "Epoch 67/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2249 - acc: 0.6253 - val_loss: 0.2601 - val_acc: 0.5376\n",
      "Epoch 68/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2246 - acc: 0.6258 - val_loss: 0.2601 - val_acc: 0.5392\n",
      "Epoch 69/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2243 - acc: 0.6266 - val_loss: 0.2604 - val_acc: 0.5388\n",
      "Epoch 70/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2242 - acc: 0.6271 - val_loss: 0.2607 - val_acc: 0.5378\n",
      "Epoch 71/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2239 - acc: 0.6278 - val_loss: 0.2612 - val_acc: 0.5392\n",
      "Epoch 72/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2237 - acc: 0.6287 - val_loss: 0.2615 - val_acc: 0.5401\n",
      "Epoch 73/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2235 - acc: 0.6295 - val_loss: 0.2613 - val_acc: 0.5416\n",
      "Epoch 74/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2233 - acc: 0.6296 - val_loss: 0.2611 - val_acc: 0.5405\n",
      "Epoch 75/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2231 - acc: 0.6301 - val_loss: 0.2623 - val_acc: 0.5379\n",
      "Epoch 76/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2228 - acc: 0.6308 - val_loss: 0.2625 - val_acc: 0.5361\n",
      "Epoch 77/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2226 - acc: 0.6317 - val_loss: 0.2623 - val_acc: 0.5384\n",
      "Epoch 78/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2225 - acc: 0.6318 - val_loss: 0.2623 - val_acc: 0.5363\n",
      "Epoch 79/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2223 - acc: 0.6324 - val_loss: 0.2625 - val_acc: 0.5365\n",
      "Epoch 80/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2221 - acc: 0.6331 - val_loss: 0.2626 - val_acc: 0.5393\n",
      "Epoch 81/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2219 - acc: 0.6337 - val_loss: 0.2629 - val_acc: 0.5375\n",
      "Epoch 82/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2217 - acc: 0.6341 - val_loss: 0.2638 - val_acc: 0.5401\n",
      "Epoch 83/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2216 - acc: 0.6346 - val_loss: 0.2636 - val_acc: 0.5349\n",
      "Epoch 84/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2214 - acc: 0.6351 - val_loss: 0.2634 - val_acc: 0.5376\n",
      "Epoch 85/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2212 - acc: 0.6355 - val_loss: 0.2641 - val_acc: 0.5369\n",
      "Epoch 86/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2210 - acc: 0.6363 - val_loss: 0.2646 - val_acc: 0.5373\n",
      "Epoch 87/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2209 - acc: 0.6365 - val_loss: 0.2641 - val_acc: 0.5381\n",
      "Epoch 88/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2207 - acc: 0.6370 - val_loss: 0.2653 - val_acc: 0.5368\n",
      "Epoch 89/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2205 - acc: 0.6376 - val_loss: 0.2655 - val_acc: 0.5385\n",
      "Epoch 90/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2203 - acc: 0.6376 - val_loss: 0.2654 - val_acc: 0.5349\n",
      "Epoch 91/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2202 - acc: 0.6381 - val_loss: 0.2659 - val_acc: 0.5360\n",
      "Epoch 92/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2200 - acc: 0.6390 - val_loss: 0.2652 - val_acc: 0.5367\n",
      "Epoch 93/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2199 - acc: 0.6391 - val_loss: 0.2659 - val_acc: 0.5393\n",
      "Epoch 94/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2197 - acc: 0.6395 - val_loss: 0.2659 - val_acc: 0.5370\n",
      "Epoch 95/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2196 - acc: 0.6400 - val_loss: 0.2669 - val_acc: 0.5353\n",
      "Epoch 96/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2194 - acc: 0.6406 - val_loss: 0.2658 - val_acc: 0.5358\n",
      "Epoch 97/200\n",
      "1626098/1626098 [==============================] - 90s 55us/step - loss: 0.2193 - acc: 0.6406 - val_loss: 0.2669 - val_acc: 0.5373\n",
      "Epoch 98/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2191 - acc: 0.6412 - val_loss: 0.2669 - val_acc: 0.5371\n",
      "Epoch 99/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2189 - acc: 0.6417 - val_loss: 0.2672 - val_acc: 0.5345\n",
      "Epoch 100/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2188 - acc: 0.6419 - val_loss: 0.2671 - val_acc: 0.5389\n",
      "Epoch 101/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2187 - acc: 0.6423 - val_loss: 0.2671 - val_acc: 0.5368\n",
      "Epoch 102/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2185 - acc: 0.6430 - val_loss: 0.2673 - val_acc: 0.5365\n",
      "Epoch 103/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2184 - acc: 0.6433 - val_loss: 0.2683 - val_acc: 0.5341\n",
      "Epoch 104/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2183 - acc: 0.6436 - val_loss: 0.2676 - val_acc: 0.5368\n",
      "Epoch 105/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2181 - acc: 0.6440 - val_loss: 0.2684 - val_acc: 0.5342\n",
      "Epoch 106/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2180 - acc: 0.6444 - val_loss: 0.2679 - val_acc: 0.5373\n",
      "Epoch 107/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2178 - acc: 0.6444 - val_loss: 0.2687 - val_acc: 0.5331\n",
      "Epoch 108/200\n",
      "1626098/1626098 [==============================] - 91s 56us/step - loss: 0.2177 - acc: 0.6451 - val_loss: 0.2694 - val_acc: 0.5362\n",
      "Epoch 109/200\n",
      "1626098/1626098 [==============================] - 93s 57us/step - loss: 0.2176 - acc: 0.6453 - val_loss: 0.2680 - val_acc: 0.5357\n",
      "Epoch 110/200\n",
      "1626098/1626098 [==============================] - 91s 56us/step - loss: 0.2175 - acc: 0.6456 - val_loss: 0.2684 - val_acc: 0.5345\n",
      "Epoch 111/200\n",
      "1626098/1626098 [==============================] - 95s 59us/step - loss: 0.2173 - acc: 0.6462 - val_loss: 0.2691 - val_acc: 0.5337\n",
      "Epoch 112/200\n",
      "1626098/1626098 [==============================] - 93s 57us/step - loss: 0.2172 - acc: 0.6460 - val_loss: 0.2693 - val_acc: 0.5341\n",
      "Epoch 113/200\n",
      "1626098/1626098 [==============================] - 95s 58us/step - loss: 0.2171 - acc: 0.6462 - val_loss: 0.2695 - val_acc: 0.5352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/200\n",
      "1626098/1626098 [==============================] - 96s 59us/step - loss: 0.2170 - acc: 0.6470 - val_loss: 0.2703 - val_acc: 0.5336\n",
      "Epoch 115/200\n",
      "1626098/1626098 [==============================] - 97s 60us/step - loss: 0.2168 - acc: 0.6474 - val_loss: 0.2695 - val_acc: 0.5350\n",
      "Epoch 116/200\n",
      "1626098/1626098 [==============================] - 92s 57us/step - loss: 0.2168 - acc: 0.6475 - val_loss: 0.2698 - val_acc: 0.5343\n",
      "Epoch 117/200\n",
      "1626098/1626098 [==============================] - 96s 59us/step - loss: 0.2166 - acc: 0.6479 - val_loss: 0.2699 - val_acc: 0.5355\n",
      "Epoch 118/200\n",
      "1626098/1626098 [==============================] - 100s 62us/step - loss: 0.2165 - acc: 0.6483 - val_loss: 0.2700 - val_acc: 0.5356\n",
      "Epoch 119/200\n",
      "1626098/1626098 [==============================] - 91s 56us/step - loss: 0.2164 - acc: 0.6485 - val_loss: 0.2701 - val_acc: 0.5349\n",
      "Epoch 120/200\n",
      "1626098/1626098 [==============================] - 91s 56us/step - loss: 0.2163 - acc: 0.6493 - val_loss: 0.2697 - val_acc: 0.5380\n",
      "Epoch 121/200\n",
      "1626098/1626098 [==============================] - 92s 57us/step - loss: 0.2162 - acc: 0.6489 - val_loss: 0.2709 - val_acc: 0.5353\n",
      "Epoch 122/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2161 - acc: 0.6493 - val_loss: 0.2701 - val_acc: 0.5357\n",
      "Epoch 123/200\n",
      "1626098/1626098 [==============================] - 87s 54us/step - loss: 0.2159 - acc: 0.6497 - val_loss: 0.2700 - val_acc: 0.5364\n",
      "Epoch 124/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2159 - acc: 0.6498 - val_loss: 0.2710 - val_acc: 0.5343\n",
      "Epoch 125/200\n",
      "1626098/1626098 [==============================] - 93s 57us/step - loss: 0.2157 - acc: 0.6502 - val_loss: 0.2709 - val_acc: 0.5336\n",
      "Epoch 126/200\n",
      "1626098/1626098 [==============================] - 91s 56us/step - loss: 0.2156 - acc: 0.6509 - val_loss: 0.2716 - val_acc: 0.5338\n",
      "Epoch 127/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2155 - acc: 0.6506 - val_loss: 0.2712 - val_acc: 0.5344\n",
      "Epoch 128/200\n",
      "1626098/1626098 [==============================] - 90s 55us/step - loss: 0.2154 - acc: 0.6507 - val_loss: 0.2713 - val_acc: 0.5351\n",
      "Epoch 129/200\n",
      "1626098/1626098 [==============================] - 86s 53us/step - loss: 0.2153 - acc: 0.6514 - val_loss: 0.2720 - val_acc: 0.5335\n",
      "Epoch 130/200\n",
      "1626098/1626098 [==============================] - 84s 52us/step - loss: 0.2152 - acc: 0.6519 - val_loss: 0.2723 - val_acc: 0.5353\n",
      "Epoch 131/200\n",
      "1626098/1626098 [==============================] - 84s 52us/step - loss: 0.2151 - acc: 0.6518 - val_loss: 0.2721 - val_acc: 0.5320\n",
      "Epoch 132/200\n",
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2151 - acc: 0.6517 - val_loss: 0.2725 - val_acc: 0.5338\n",
      "Epoch 133/200\n",
      "1626098/1626098 [==============================] - 84s 52us/step - loss: 0.2149 - acc: 0.6524 - val_loss: 0.2719 - val_acc: 0.5354\n",
      "Epoch 134/200\n",
      "1626098/1626098 [==============================] - 84s 52us/step - loss: 0.2148 - acc: 0.6524 - val_loss: 0.2731 - val_acc: 0.5368\n",
      "Epoch 135/200\n",
      "1626098/1626098 [==============================] - 84s 52us/step - loss: 0.2147 - acc: 0.6527 - val_loss: 0.2718 - val_acc: 0.5349\n",
      "Epoch 136/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2146 - acc: 0.6527 - val_loss: 0.2720 - val_acc: 0.5329\n",
      "Epoch 137/200\n",
      "1626098/1626098 [==============================] - 90s 55us/step - loss: 0.2146 - acc: 0.6533 - val_loss: 0.2716 - val_acc: 0.5340\n",
      "Epoch 138/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2144 - acc: 0.6534 - val_loss: 0.2731 - val_acc: 0.5327\n",
      "Epoch 139/200\n",
      "1626098/1626098 [==============================] - 90s 55us/step - loss: 0.2143 - acc: 0.6540 - val_loss: 0.2721 - val_acc: 0.5338\n",
      "Epoch 140/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2143 - acc: 0.6536 - val_loss: 0.2730 - val_acc: 0.5344\n",
      "Epoch 141/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2142 - acc: 0.6540 - val_loss: 0.2729 - val_acc: 0.5339\n",
      "Epoch 142/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2141 - acc: 0.6543 - val_loss: 0.2730 - val_acc: 0.5329\n",
      "Epoch 143/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2140 - acc: 0.6544 - val_loss: 0.2735 - val_acc: 0.5346\n",
      "Epoch 144/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2139 - acc: 0.6544 - val_loss: 0.2731 - val_acc: 0.5345\n",
      "Epoch 145/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2138 - acc: 0.6551 - val_loss: 0.2734 - val_acc: 0.5353\n",
      "Epoch 146/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2137 - acc: 0.6553 - val_loss: 0.2728 - val_acc: 0.5326\n",
      "Epoch 147/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2136 - acc: 0.6552 - val_loss: 0.2734 - val_acc: 0.5323\n",
      "Epoch 148/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2135 - acc: 0.6555 - val_loss: 0.2739 - val_acc: 0.5352\n",
      "Epoch 149/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2135 - acc: 0.6558 - val_loss: 0.2733 - val_acc: 0.5330\n",
      "Epoch 150/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2134 - acc: 0.6563 - val_loss: 0.2738 - val_acc: 0.5334\n",
      "Epoch 151/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2133 - acc: 0.6568 - val_loss: 0.2731 - val_acc: 0.5358\n",
      "Epoch 152/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2132 - acc: 0.6567 - val_loss: 0.2741 - val_acc: 0.5338\n",
      "Epoch 153/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2131 - acc: 0.6570 - val_loss: 0.2744 - val_acc: 0.5343\n",
      "Epoch 154/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2130 - acc: 0.6568 - val_loss: 0.2739 - val_acc: 0.5333\n",
      "Epoch 155/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2129 - acc: 0.6576 - val_loss: 0.2741 - val_acc: 0.5354\n",
      "Epoch 156/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2129 - acc: 0.6575 - val_loss: 0.2748 - val_acc: 0.5332\n",
      "Epoch 157/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2128 - acc: 0.6577 - val_loss: 0.2749 - val_acc: 0.5343\n",
      "Epoch 158/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2127 - acc: 0.6578 - val_loss: 0.2754 - val_acc: 0.5330\n",
      "Epoch 159/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2125 - acc: 0.6582 - val_loss: 0.2744 - val_acc: 0.5344\n",
      "Epoch 160/200\n",
      "1626098/1626098 [==============================] - 89s 54us/step - loss: 0.2125 - acc: 0.6584 - val_loss: 0.2751 - val_acc: 0.5347\n",
      "Epoch 161/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2125 - acc: 0.6583 - val_loss: 0.2750 - val_acc: 0.5340\n",
      "Epoch 162/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2123 - acc: 0.6591 - val_loss: 0.2757 - val_acc: 0.5318\n",
      "Epoch 163/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2123 - acc: 0.6586 - val_loss: 0.2750 - val_acc: 0.5325\n",
      "Epoch 164/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2122 - acc: 0.6594 - val_loss: 0.2755 - val_acc: 0.5330\n",
      "Epoch 165/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2122 - acc: 0.6590 - val_loss: 0.2754 - val_acc: 0.5326\n",
      "Epoch 166/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2121 - acc: 0.6593 - val_loss: 0.2758 - val_acc: 0.5306\n",
      "Epoch 167/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2120 - acc: 0.6596 - val_loss: 0.2764 - val_acc: 0.5326\n",
      "Epoch 168/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2119 - acc: 0.6601 - val_loss: 0.2767 - val_acc: 0.5298\n",
      "Epoch 169/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2118 - acc: 0.6603 - val_loss: 0.2751 - val_acc: 0.5317\n",
      "Epoch 170/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1626098/1626098 [==============================] - 85s 52us/step - loss: 0.2118 - acc: 0.6603 - val_loss: 0.2754 - val_acc: 0.5329\n",
      "Epoch 171/200\n",
      "1626098/1626098 [==============================] - 86s 53us/step - loss: 0.2117 - acc: 0.6604 - val_loss: 0.2763 - val_acc: 0.5319\n",
      "Epoch 172/200\n",
      "1626098/1626098 [==============================] - 87s 54us/step - loss: 0.2117 - acc: 0.6603 - val_loss: 0.2759 - val_acc: 0.5348\n",
      "Epoch 173/200\n",
      "1626098/1626098 [==============================] - 87s 54us/step - loss: 0.2116 - acc: 0.6606 - val_loss: 0.2761 - val_acc: 0.5324\n",
      "Epoch 174/200\n",
      "1626098/1626098 [==============================] - 87s 54us/step - loss: 0.2115 - acc: 0.6608 - val_loss: 0.2766 - val_acc: 0.5323\n",
      "Epoch 175/200\n",
      "1626098/1626098 [==============================] - 87s 53us/step - loss: 0.2114 - acc: 0.6609 - val_loss: 0.2773 - val_acc: 0.5323\n",
      "Epoch 176/200\n",
      "1626098/1626098 [==============================] - 86s 53us/step - loss: 0.2114 - acc: 0.6615 - val_loss: 0.2770 - val_acc: 0.5334\n",
      "Epoch 177/200\n",
      "1626098/1626098 [==============================] - 86s 53us/step - loss: 0.2113 - acc: 0.6615 - val_loss: 0.2763 - val_acc: 0.5330\n",
      "Epoch 178/200\n",
      "1626098/1626098 [==============================] - 86s 53us/step - loss: 0.2112 - acc: 0.6616 - val_loss: 0.2768 - val_acc: 0.5343\n",
      "Epoch 179/200\n",
      "1626098/1626098 [==============================] - 86s 53us/step - loss: 0.2111 - acc: 0.6617 - val_loss: 0.2777 - val_acc: 0.5315\n",
      "Epoch 180/200\n",
      "1626098/1626098 [==============================] - 86s 53us/step - loss: 0.2111 - acc: 0.6623 - val_loss: 0.2772 - val_acc: 0.5317\n",
      "Epoch 181/200\n",
      "1626098/1626098 [==============================] - 86s 53us/step - loss: 0.2110 - acc: 0.6620 - val_loss: 0.2771 - val_acc: 0.5324\n",
      "Epoch 182/200\n",
      "1626098/1626098 [==============================] - 86s 53us/step - loss: 0.2110 - acc: 0.6625 - val_loss: 0.2767 - val_acc: 0.5324\n",
      "Epoch 183/200\n",
      "1626098/1626098 [==============================] - 86s 53us/step - loss: 0.2109 - acc: 0.6621 - val_loss: 0.2775 - val_acc: 0.5333\n",
      "Epoch 184/200\n",
      "1626098/1626098 [==============================] - 86s 53us/step - loss: 0.2109 - acc: 0.6627 - val_loss: 0.2776 - val_acc: 0.5332\n",
      "Epoch 185/200\n",
      "1626098/1626098 [==============================] - 87s 53us/step - loss: 0.2108 - acc: 0.6626 - val_loss: 0.2774 - val_acc: 0.5334\n",
      "Epoch 186/200\n",
      "1626098/1626098 [==============================] - 90s 55us/step - loss: 0.2107 - acc: 0.6632 - val_loss: 0.2777 - val_acc: 0.5332\n",
      "Epoch 187/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2106 - acc: 0.6630 - val_loss: 0.2773 - val_acc: 0.5335\n",
      "Epoch 188/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2106 - acc: 0.6633 - val_loss: 0.2787 - val_acc: 0.5293\n",
      "Epoch 189/200\n",
      "1626098/1626098 [==============================] - 90s 55us/step - loss: 0.2105 - acc: 0.6638 - val_loss: 0.2776 - val_acc: 0.5323\n",
      "Epoch 190/200\n",
      "1626098/1626098 [==============================] - 90s 55us/step - loss: 0.2104 - acc: 0.6634 - val_loss: 0.2782 - val_acc: 0.5349\n",
      "Epoch 191/200\n",
      "1626098/1626098 [==============================] - 90s 55us/step - loss: 0.2104 - acc: 0.6640 - val_loss: 0.2787 - val_acc: 0.5329\n",
      "Epoch 192/200\n",
      "1626098/1626098 [==============================] - 89s 55us/step - loss: 0.2103 - acc: 0.6643 - val_loss: 0.2778 - val_acc: 0.5318\n",
      "Epoch 193/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2103 - acc: 0.6642 - val_loss: 0.2786 - val_acc: 0.5307\n",
      "Epoch 194/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2102 - acc: 0.6639 - val_loss: 0.2780 - val_acc: 0.5337\n",
      "Epoch 195/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2101 - acc: 0.6647 - val_loss: 0.2782 - val_acc: 0.5315\n",
      "Epoch 196/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2100 - acc: 0.6646 - val_loss: 0.2785 - val_acc: 0.5316\n",
      "Epoch 197/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2101 - acc: 0.6646 - val_loss: 0.2782 - val_acc: 0.5332\n",
      "Epoch 198/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2099 - acc: 0.6648 - val_loss: 0.2780 - val_acc: 0.5332\n",
      "Epoch 199/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2099 - acc: 0.6651 - val_loss: 0.2785 - val_acc: 0.5315\n",
      "Epoch 200/200\n",
      "1626098/1626098 [==============================] - 88s 54us/step - loss: 0.2099 - acc: 0.6652 - val_loss: 0.2786 - val_acc: 0.5313\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd479f1d748>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Dense(250, activation='relu', input_shape=(X.shape[1],)))\n",
    "model3.add(Dense(250, activation='relu'))\n",
    "model3.add(Dense(250, activation='relu'))\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model3.compile(optimizer=adam, loss='mean_squared_error', metrics=['accuracy'])\n",
    "model3.fit(X_train, y_train, epochs=200, validation_split=0.3, callbacks=[early_stop], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(dense_layers, activation, dropout):\n",
    "    '''Creates a multi-layer perceptron model\n",
    "    \n",
    "    dense_layers: List of layer sizes; one number per layer\n",
    "    '''\n",
    "\n",
    "    model = Sequential()\n",
    "    for i, layer_size in enumerate(dense_layers, 1):\n",
    "        if i == 1:\n",
    "            model.add(Dense(layer_size, input_dim=X.shape[1]))\n",
    "            model.add(Activation(activation))\n",
    "        else:\n",
    "            model.add(Dense(layer_size))\n",
    "            model.add(Activation(activation))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='Adam',\n",
    "                  metrics=['binary_accuracy', auc_roc])\n",
    "\n",
    "    return model\n",
    "\n",
    "def auc_roc(y_true, y_pred):\n",
    "    # any tensorflow metric\n",
    "    value, update_op = tf.metrics.auc(y_true, y_pred)\n",
    "\n",
    "    # find all variables created for this metric\n",
    "    metric_vars = [i for i in tf.local_variables() if 'auc_roc' in i.name.split('/')[1]]\n",
    "\n",
    "    # Add metric variables to GLOBAL_VARIABLES collection.\n",
    "    # They will be initialized for new session.\n",
    "    for v in metric_vars:\n",
    "        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n",
    "\n",
    "    # force to update metric values\n",
    "    with tf.control_dependencies([update_op]):\n",
    "        value = tf.identity(value)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 42 candidates, totalling 210 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "ename": "TerminatedWorkerError",
     "evalue": "A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker. The exit codes of the workers are {SIGABRT(-6)}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-386f4dc409ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mearly_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auc_roc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mgrid_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nBest Score: {:.2%}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nBest Params:\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker. The exit codes of the workers are {SIGABRT(-6)}"
     ]
    }
   ],
   "source": [
    "param_grid = {'dense_layers': [[32], [32, 32], [64], [64, 64], [64, 64, 32], [64, 32], [128]],\n",
    "              'activation'  : ['relu', 'tanh'],\n",
    "              'dropout'     : [.25, .5, .75],\n",
    "              }\n",
    "\n",
    "clf = KerasClassifier(make_model, epochs=10, batch_size=128)\n",
    "grid_cv = GridSearchCV(clf, param_grid, cv=5, refit=True, return_train_score=True, n_jobs=-1, verbose=1, iid=False, error_score=np.nan, scoring='roc_auc')\n",
    "early_stop = EarlyStopping(monitor='auc_roc', patience=300, verbose=1, mode='max')\n",
    "\n",
    "grid_cv.fit(X_train, y_train, callbacks=[early_stop], verbose=2, epochs=50)\n",
    "print('\\nBest Score: {:.2%}'.format(grid_cv.best_score_))\n",
    "print('\\nBest Params:\\n', pd.Series(grid_cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = grid_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ballmdr/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/ballmdr/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "-1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4374\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 4375\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   4376\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index._bin_search\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'int' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-ff776af6539b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreturn_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-a6cea5c9e936>\u001b[0m in \u001b[0;36mreturn_plot\u001b[0;34m(estimator, plot)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Startegy_returns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Returns'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Predicted_Signal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mCumulative_Strategy_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Startegy_returns'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Return: {}%'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCumulative_Strategy_returns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4393\u001b[0m             \u001b[0;31m# python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4394\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4395\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4396\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: -1"
     ]
    }
   ],
   "source": [
    "return_plot(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.872215911120946\n"
     ]
    }
   ],
   "source": [
    "y_pred = final_model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('h5/keras_data2_M1_53_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
